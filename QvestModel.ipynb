{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44268a4f-d7bb-463a-aa77-a37204bca75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 00:17:05.827134: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-05 00:17:06.197677: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-05 00:17:06.533655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738714626.813107   24683 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738714626.889844   24683 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-05 00:17:07.625347: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Flatten, Concatenate, Dropout, Normalization, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ba1f5-c094-47f8-9f3b-ccfa46092c4c",
   "metadata": {},
   "source": [
    "###  1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4c0787-76ff-4580-8bd6-ba9bf58c71c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title_id', 'quote_id', 'qli_fee', 'qli_start_date', 'qli_end_date',\n",
      "       'qli_media_type', 'qli_rights', 'relevance', 'opportunity_id',\n",
      "       'opportunity_name', 'opportunity_deal_amount', 'opp_start_date',\n",
      "       'opp_expected_close_date', 'opportunity_geography_type',\n",
      "       'opportunity_reporting_territory', 'opportunity_stage', 'title_name',\n",
      "       'title_r', 'title_runtime', 'title_primary_genre', 'title_level',\n",
      "       'title_production_year', 'title_synopsis'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV FILES\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ff308-02fc-4c65-bb7c-0706c521343b",
   "metadata": {},
   "source": [
    "### 2. Define Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4b9c39-9593-4809-ace2-f0dfb3f9239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "#opportunity_name\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"] \n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Target: title_id (which title best fits the opportunity)\n",
    "target = \"title_name\"\n",
    "\n",
    "# TO DO: CHECK if there's a better fit for title_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258bceb-effe-4d99-8542-41614ccbf6bd",
   "metadata": {},
   "source": [
    "### 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d696d090-7df6-4fa3-8de2-a8e339af3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "#label_encoders = {}\n",
    "#for col in categorical_features:\n",
    "#    le = LabelEncoder()\n",
    "#    merged_df[col] = le.fit_transform(merged_df[col])\n",
    "#    label_encoders[col] = le\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "merged_df[numerical_features] = scaler.fit_transform(merged_df[numerical_features])\n",
    "\n",
    "# Handle date features\n",
    "for col in date_features:\n",
    "    merged_df[col] = pd.to_datetime(merged_df[col])\n",
    "    merged_df[col] = (merged_df[col] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Handle text features\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "sequences = tokenizer.texts_to_sequences(merged_df['title_synopsis'])\n",
    "max_len = 100\n",
    "text_data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Separate text data from other features\n",
    "X_other = merged_df[categorical_features + numerical_features + date_features]\n",
    "y = merged_df['relevance']\n",
    "\n",
    "# Split data\n",
    "X_train_other, X_test_other, X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_other, text_data, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434c1177-016c-4270-a219-cfad61189d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1730c8a-a00f-4f2c-9f60-dd47336b49da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(label_encoders, 'code/label_encoders.pkl')\n",
    "joblib.dump(scaler, 'code/scaler.pkl')\n",
    "joblib.dump(tokenizer, 'code/tokenizer.pkl')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f541b-f487-42cd-ad22-49a9b563c51c",
   "metadata": {},
   "source": [
    "### 4. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e07f9b-287d-46cf-9d43-a8632671677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 00:17:51.082621: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 195ms/step - accuracy: 0.0000e+00 - loss: 105442072.0000 - val_accuracy: 0.0000e+00 - val_loss: 5204688.5000\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 90ms/step - accuracy: 0.0000e+00 - loss: 20569134.0000 - val_accuracy: 0.0000e+00 - val_loss: 48677884.0000\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 95ms/step - accuracy: 0.0000e+00 - loss: 46308464.0000 - val_accuracy: 0.0000e+00 - val_loss: 16327309.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 85ms/step - accuracy: 0.0000e+00 - loss: 13355116.0000 - val_accuracy: 0.0000e+00 - val_loss: 34777064.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 140ms/step - accuracy: 0.0000e+00 - loss: 33584808.0000 - val_accuracy: 0.0000e+00 - val_loss: 22317418.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.0000e+00 - loss: 14462325.0000 - val_accuracy: 0.0000e+00 - val_loss: 12262901.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - accuracy: 0.0000e+00 - loss: 11698403.0000 - val_accuracy: 0.0000e+00 - val_loss: 2694806.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 122ms/step - accuracy: 0.0000e+00 - loss: 4836194.5000 - val_accuracy: 0.0000e+00 - val_loss: 862282.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - accuracy: 0.0000e+00 - loss: 5268887.0000 - val_accuracy: 0.0000e+00 - val_loss: 8456155.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 117ms/step - accuracy: 0.0000e+00 - loss: 6732512.0000 - val_accuracy: 0.0000e+00 - val_loss: 9303672.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f74a136ed90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# Define model\n",
    "input_cat = Input(shape=(len(categorical_features),), name=\"input_cat\")\n",
    "input_num = Input(shape=(len(numerical_features) + len(date_features),), name=\"input_num\")\n",
    "input_text = Input(shape=(max_len,), name=\"input_text\")\n",
    "\n",
    "embedding = Embedding(input_dim=10000, output_dim=128)(input_text)\n",
    "lstm = LSTM(64)(embedding)\n",
    "\n",
    "concat = Concatenate()([input_cat, input_num, lstm])\n",
    "dense1 = Dense(128, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "inputs_array = [input_cat, input_num, input_text]\n",
    "\n",
    "model = keras.Model(inputs=[input_cat, input_num, input_text], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    [X_train_other[categorical_features], X_train_other[numerical_features + date_features], X_train_text],\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6c06af3-e6ab-41f5-83e1-a41354675ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_text          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280,000</span> │ input_text[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_cat           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_num           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">76</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_cat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ input_num[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,856</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_text          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │  \u001b[38;5;34m1,280,000\u001b[0m │ input_text[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_cat           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_num           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m49,408\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m76\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_cat[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ input_num[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │      \u001b[38;5;34m9,856\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m129\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,018,181</span> (15.33 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,018,181\u001b[0m (15.33 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,339,393</span> (5.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,339,393\u001b[0m (5.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,678,788</span> (10.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,678,788\u001b[0m (10.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836f3be-a104-4dcd-b20a-9fff6cec4f30",
   "metadata": {},
   "source": [
    "## 5. Deploy to AWS SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ff49c05-0e8c-420d-8b58-8897b633245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save the model\n",
    "#model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68378730-167e-40ff-8b82-fdda2fd7692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '1/'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 5), dtype=tf.float32, name='input_cat'), TensorSpec(shape=(None, 7), dtype=tf.float32, name='input_num'), TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_text')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140138897453520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897453136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897456592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897456784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897453904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897456976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897455632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140138897456016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model saved in SavedModel format.\n",
      "['assets', 'variables', 'fingerprint.pb', 'saved_model.pb']\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "#model.save('model.h5')\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "#tf.saved_model.save(model, 'model') #saved model\n",
    "# Define input signatures\n",
    "input_signatures = {\n",
    "    \"input_cat\": tf.TensorSpec(shape=(None, len(categorical_features)), dtype=tf.float32, name=\"input_cat\"),\n",
    "    \"input_num\": tf.TensorSpec(shape=(None, len(numerical_features) + len(date_features)), dtype=tf.float32, name=\"input_num\"),\n",
    "    \"input_text\": tf.TensorSpec(shape=(None, max_len), dtype=tf.int32, name=\"input_text\"),\n",
    "}\n",
    "\n",
    "#model.export(\"1/\") \n",
    "# Export the model\n",
    "#tf.saved_model.save(model, '1/')\n",
    "model.export(\n",
    "    \"1/\"\n",
    "    #input_tensors=input_signatures\n",
    ")\n",
    "print(\"Model saved in SavedModel format.\")\n",
    "\n",
    "# Verify saved structure\n",
    "import os\n",
    "print(os.listdir(\"1/\")) # Should show [\"saved_model.pb\", \"variables\", \"assets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9db23712-0669-4266-947b-522bea7c22ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "Keras Version: 3.8.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(\"Keras Version:\", tf.keras.__version__)\n",
    "\n",
    "#print(model.framework_version)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4770027-e520-49fb-9bfb-618787dfeebf",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input signatures\n",
    "input_signatures = {\n",
    "    \"input_cat\": tf.TensorSpec(shape=(None, len(categorical_features)), dtype=tf.float32, name=\"input_cat\"),\n",
    "    \"input_num\": tf.TensorSpec(shape=(None, len(numerical_features) + len(date_features)), dtype=tf.float32, name=\"input_num\"),\n",
    "    \"input_text\": tf.TensorSpec(shape=(None, max_len), dtype=tf.int32, name=\"input_text\"),\n",
    "}\n",
    "\n",
    "# Create a concrete function using model.__call__\n",
    "concrete_function = model.__call__.get_concrete_function(**input_signatures)\n",
    "\n",
    "# Export the model\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    \"1/\",  # Directory to save the model\n",
    "    signatures={\n",
    "        \"serving_default\": concrete_function  # Define the serving signature\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model saved in SavedModel format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8517f071-2f6c-4ea6-964f-d4faf45b76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Create a tar.gz file\n",
    "#with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "#    tar.add('model', arcname=os.path.basename('1'))\n",
    "\n",
    "#print(\"Model packaged into model.tar.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d399deac-a9e9-45e2-8561-51f9867a0734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tar.gz created successfully.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# List of files to include in the model.tar.gz\n",
    "files_to_include = [\n",
    "    '1/',  # TensorFlow SavedModel directory saved_model\n",
    "    #'model.h5',\n",
    "    'code/',\n",
    "    #'inference.py',  # Custom inference script\n",
    "    #'requirements.txt',\n",
    "    #'label_encoders.pkl',  # Preprocessing object\n",
    "    #'scaler.pkl',  # Preprocessing object\n",
    "    #'tokenizer.pkl',  # Preprocessing object\n",
    "    #'titles.csv'\n",
    "]\n",
    "\n",
    "# Create the model.tar.gz file\n",
    "#with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "#    for file in files_to_include:\n",
    "#        tar.add(file)\n",
    "\n",
    "\n",
    "# Create the model.tar.gz file\n",
    "with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "    for file in files_to_include:\n",
    "        # If it's the model directory, rename it to just '1' inside the tarball\n",
    "        #if file == '1/':\n",
    "        #    tar.add(file, arcname='1')\n",
    "        #if file == 'code/':\n",
    "        #    tar.add(file, arcname='code')\n",
    "        #else:\n",
    "            tar.add(file)\n",
    "            \n",
    "print(\"model.tar.gz created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832484b6-e727-49f6-9b13-a00693178b44",
   "metadata": {},
   "source": [
    "# 6. Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b79aa8-c884-41a5-afca-c7b322888b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to s3://titlesbucket/opportunity-title-prediction/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'titlesbucket'\n",
    "key = 'opportunity-title-prediction/model.tar.gz'\n",
    "\n",
    "# Upload the file\n",
    "#opportunity-title-prediction/model.tar.gz\n",
    "s3.upload_file('model.tar.gz', bucket_name, key)\n",
    "\n",
    "print(f\"Model uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91e47098-13a8-46cd-be71-9b8eeb2f30fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nestimator = TensorFlow(\\n    entry_point=\\'inference.py\\',\\n    source_dir=\\'code\\',\\n    instance_count=1,\\n    instance_type=\"ml.c4.xlarge\",\\n    framework_version=\"2.2\",\\n    py_version=\"py37\",\\n    name=\\'Titles-Prediction-Model-23\\',\\n    role = role\\n)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Path to your new model.tar.gz in S3\n",
    "# opportunity-title-prediction\n",
    "model_data = \"s3://titlesbucket/opportunity-title-prediction/model.tar.gz\"\n",
    "\n",
    "# Instantiate a TensorFlow model\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    framework_version='2.18.0',  # TensorFlow version\n",
    "    name='Titles-Prediction-Model-22',\n",
    "    #py_version=\"py37\",\n",
    ")\n",
    "\n",
    "#tensorflow_model.fit(inputs)\n",
    "print(tensorflow_model.framework_version)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "estimator = TensorFlow(\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    framework_version=\"2.2\",\n",
    "    py_version=\"py37\",\n",
    "    name='Titles-Prediction-Model-23',\n",
    "    role = role\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "561c353c-ccea-478f-a815-0df22901a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::779846812857:role/service-role/AmazonSageMaker-ExecutionRole-20250126T233747\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2a30e-0357-4659-aeed-2c9ad6db30d9",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a190eb8e-c5b4-4388-ad6a-777edf0b0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "#sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Delete the existing endpoint configuration\n",
    "#sagemaker_client.delete_endpoint_config(EndpointConfigName='titles-prediction-my-endpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65084b1a-8810-4cd1-a527-beeb5d06a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      5\u001b[0m ENDPOINT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitles-Prediction-Model-23-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS-\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mtensorflow_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.c5.large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENDPOINT_NAME\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#update_endpoint=True\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel deployment finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictor)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tensorflow/model.py:364\u001b[0m, in \u001b[0;36mTensorFlowModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TensorFlow version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support EIA.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework_version\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensorFlowModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/model.py:1749\u001b[0m, in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m if endpoint_name:\n\u001b[1;32m   1748\u001b[0m     self.endpoint_name = endpoint_name\n\u001b[0;32m-> 1749\u001b[0m else:\n\u001b[1;32m   1750\u001b[0m     base_endpoint_name = self._base_name or utils.base_from_name(self.name)\n\u001b[1;32m   1751\u001b[0m     if self._is_compiled_model and not is_serverless:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5728\u001b[0m, in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_capture_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5726\u001b[0m     data_capture_config_dict \u001b[38;5;241m=\u001b[39m data_capture_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 5728\u001b[0m _create_resource(\n\u001b[1;32m   5729\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\n\u001b[1;32m   5730\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   5731\u001b[0m         model_name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m   5732\u001b[0m         initial_instance_count\u001b[38;5;241m=\u001b[39minitial_instance_count,\n\u001b[1;32m   5733\u001b[0m         instance_type\u001b[38;5;241m=\u001b[39minstance_type,\n\u001b[1;32m   5734\u001b[0m         accelerator_type\u001b[38;5;241m=\u001b[39maccelerator_type,\n\u001b[1;32m   5735\u001b[0m         data_capture_config_dict\u001b[38;5;241m=\u001b[39mdata_capture_config_dict,\n\u001b[1;32m   5736\u001b[0m         tags\u001b[38;5;241m=\u001b[39mendpoint_config_tags,\n\u001b[1;32m   5737\u001b[0m     )\n\u001b[1;32m   5738\u001b[0m )\n\u001b[1;32m   5740\u001b[0m \u001b[38;5;66;03m# to make change backwards compatible\u001b[39;00m\n\u001b[1;32m   5741\u001b[0m response \u001b[38;5;241m=\u001b[39m _create_resource(\n\u001b[1;32m   5742\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_endpoint(\n\u001b[1;32m   5743\u001b[0m         endpoint_name\u001b[38;5;241m=\u001b[39mname, config_name\u001b[38;5;241m=\u001b[39mname, tags\u001b[38;5;241m=\u001b[39mendpoint_tags, wait\u001b[38;5;241m=\u001b[39mwait\n\u001b[1;32m   5744\u001b[0m     )\n\u001b[1;32m   5745\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:4586\u001b[0m, in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_endpoint_config_from_existing\u001b[39m(\n\u001b[1;32m   4548\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4549\u001b[0m     existing_config_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4556\u001b[0m     endpoint_type\u001b[38;5;241m=\u001b[39mEndpointType\u001b[38;5;241m.\u001b[39mMODEL_BASED,\n\u001b[1;32m   4557\u001b[0m ):\n\u001b[1;32m   4558\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create an Amazon SageMaker endpoint configuration from an existing one.\u001b[39;00m\n\u001b[1;32m   4559\u001b[0m \n\u001b[1;32m   4560\u001b[0m \u001b[38;5;124;03m    It also updates any values that were passed in.\u001b[39;00m\n\u001b[1;32m   4561\u001b[0m \u001b[38;5;124;03m    The endpoint configuration identifies the Amazon SageMaker model (created using the\u001b[39;00m\n\u001b[1;32m   4562\u001b[0m \u001b[38;5;124;03m    ``CreateModel`` API) and the hardware configuration on which to deploy the model. Provide\u001b[39;00m\n\u001b[1;32m   4563\u001b[0m \u001b[38;5;124;03m    this endpoint configuration to the ``CreateEndpoint`` API, which then launches the\u001b[39;00m\n\u001b[1;32m   4564\u001b[0m \u001b[38;5;124;03m    hardware and deploys the model.\u001b[39;00m\n\u001b[1;32m   4565\u001b[0m \n\u001b[1;32m   4566\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   4567\u001b[0m \u001b[38;5;124;03m        new_config_name (str): Name of the Amazon SageMaker endpoint configuration to create.\u001b[39;00m\n\u001b[1;32m   4568\u001b[0m \u001b[38;5;124;03m        existing_config_name (str): Name of the existing Amazon SageMaker endpoint\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m \u001b[38;5;124;03m            configuration.\u001b[39;00m\n\u001b[1;32m   4570\u001b[0m \u001b[38;5;124;03m        new_tags (Optional[Tags]): Optional. The list of tags to add to the endpoint\u001b[39;00m\n\u001b[1;32m   4571\u001b[0m \u001b[38;5;124;03m            config. If not specified, the tags of the existing endpoint configuration are used.\u001b[39;00m\n\u001b[1;32m   4572\u001b[0m \u001b[38;5;124;03m            If any of the existing tags are reserved AWS ones (i.e. begin with \"aws\"),\u001b[39;00m\n\u001b[1;32m   4573\u001b[0m \u001b[38;5;124;03m            they are not carried over to the new endpoint configuration.\u001b[39;00m\n\u001b[1;32m   4574\u001b[0m \u001b[38;5;124;03m        new_kms_key (str): The KMS key that is used to encrypt the data on the storage volume\u001b[39;00m\n\u001b[1;32m   4575\u001b[0m \u001b[38;5;124;03m            attached to the instance hosting the endpoint (default: None). If not specified,\u001b[39;00m\n\u001b[1;32m   4576\u001b[0m \u001b[38;5;124;03m            the KMS key of the existing endpoint configuration is used.\u001b[39;00m\n\u001b[1;32m   4577\u001b[0m \u001b[38;5;124;03m        new_data_capture_config_dict (dict): Specifies configuration related to Endpoint data\u001b[39;00m\n\u001b[1;32m   4578\u001b[0m \u001b[38;5;124;03m            capture for use with Amazon SageMaker Model Monitoring (default: None).\u001b[39;00m\n\u001b[1;32m   4579\u001b[0m \u001b[38;5;124;03m            If not specified, the data capture configuration of the existing\u001b[39;00m\n\u001b[1;32m   4580\u001b[0m \u001b[38;5;124;03m            endpoint configuration is used.\u001b[39;00m\n\u001b[1;32m   4581\u001b[0m \u001b[38;5;124;03m        new_production_variants (list[dict]): The configuration for which model(s) to host and\u001b[39;00m\n\u001b[1;32m   4582\u001b[0m \u001b[38;5;124;03m            the resources to deploy for hosting the model(s). If not specified,\u001b[39;00m\n\u001b[1;32m   4583\u001b[0m \u001b[38;5;124;03m            the ``ProductionVariants`` of the existing endpoint configuration is used.\u001b[39;00m\n\u001b[1;32m   4584\u001b[0m \u001b[38;5;124;03m        new_explainer_config_dict (dict): Specifies configuration to enable explainers.\u001b[39;00m\n\u001b[1;32m   4585\u001b[0m \u001b[38;5;124;03m            (default: None). If not specified, the explainer configuration of the existing\u001b[39;00m\n\u001b[0;32m-> 4586\u001b[0m \u001b[38;5;124;03m            endpoint configuration is used.\u001b[39;00m\n\u001b[1;32m   4587\u001b[0m \u001b[38;5;124;03m        endpoint_type (EndpointType): The Endpoint type to determine whether model will be\u001b[39;00m\n\u001b[1;32m   4588\u001b[0m \u001b[38;5;124;03m            deployed as Amazon SageMaker Inference Component to the endpoint\u001b[39;00m\n\u001b[1;32m   4589\u001b[0m \n\u001b[1;32m   4590\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m   4591\u001b[0m \u001b[38;5;124;03m        str: Name of the endpoint point configuration created.\u001b[39;00m\n\u001b[1;32m   4592\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4593\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, new_config_name)\n\u001b[1;32m   4595\u001b[0m     existing_endpoint_config_desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mdescribe_endpoint_config(\n\u001b[1;32m   4596\u001b[0m         EndpointConfigName\u001b[38;5;241m=\u001b[39mexisting_config_name\n\u001b[1;32m   4597\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5342\u001b[0m, in \u001b[0;36mwait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5340\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tags_list:\n\u001b[1;32m   5341\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tag[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m-> 5342\u001b[0m             non_aws_tags\u001b[38;5;241m.\u001b[39mappend(tag)\n\u001b[1;32m   5343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m non_aws_tags\n\u001b[1;32m   5344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:8266\u001b[0m, in \u001b[0;36m_wait_until\u001b[0;34m(callable_fn, poll)\u001b[0m\n\u001b[1;32m   8262\u001b[0m log_group_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/aws/sagemaker/InferenceRecommendationsJobs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   8263\u001b[0m log_stream_name \u001b[38;5;241m=\u001b[39m job_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/execution\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   8265\u001b[0m initial_logs_batch \u001b[38;5;241m=\u001b[39m get_log_events_for_inference_recommender(\n\u001b[0;32m-> 8266\u001b[0m     cloudwatch_client, log_group_name, log_stream_name\n\u001b[1;32m   8267\u001b[0m )\n\u001b[1;32m   8268\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved logStream: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_stream_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from logGroup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog_group_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   8269\u001b[0m events \u001b[38;5;241m=\u001b[39m initial_logs_batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevents\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "# m5\n",
    "from datetime import datetime\n",
    "\n",
    "ENDPOINT_NAME = 'Titles-Prediction-Model-23-' + str(datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f'))\n",
    "\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.large',  \n",
    "    endpoint_name=ENDPOINT_NAME \n",
    "    #update_endpoint=True\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0aa70bdf-af08-4019-a587-ecfb8714198b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/ml/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/ml/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Should show ['1', 'code']\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/ml/model/1\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Should show model files\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/ml/model/code\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Should show inference.py, encoders, scaler, tokenizer, etc.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/ml/model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/opt/ml/model\"))  # Should show ['1', 'code']\n",
    "print(os.listdir(\"/opt/ml/model/1\"))  # Should show model files\n",
    "print(os.listdir(\"/opt/ml/model/code\"))  # Should show inference.py, encoders, scaler, tokenizer, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc1672-b685-4856-b6c3-0b1212f15235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load data from CSV files\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "\n",
    "# Features\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"]\n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Load pre-trained model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Load preprocessing objects (label encoders, scaler, tokenizer)\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merged_df[numerical_features])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "\n",
    "# Function to preprocess opportunity data\n",
    "def preprocess_opportunity_data(opportunity_data, titles_df, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer, max_len=100):\n",
    "    \"\"\"\n",
    "    Preprocess opportunity data and combine it with each title's features.\n",
    "    Handles unseen categorical labels by mapping them to an \"unknown\" category.\n",
    "    \"\"\"\n",
    "    # Preprocess opportunity data\n",
    "    X_cat = []\n",
    "    for col in categorical_features:\n",
    "        le = label_encoders[col]\n",
    "        # Handle unseen labels by mapping them to \"unknown\"\n",
    "        if opportunity_data.get(col, \"unknown\") in le.classes_:\n",
    "            X_cat.append(le.transform([opportunity_data.get(col, \"unknown\")])[0])\n",
    "        else:\n",
    "            # Map unseen labels to \"unknown\"\n",
    "            X_cat.append(le.transform([\"unknown\"])[0])\n",
    "    X_cat = np.array(X_cat).reshape(1, -1)  # Shape: (1, num_categorical_features)\n",
    "\n",
    "    X_num = np.array([[opportunity_data.get(col, 0) for col in numerical_features]])\n",
    "    X_num = scaler.transform(X_num)  # Scale numerical features\n",
    "\n",
    "    for col in date_features:\n",
    "        date_value = datetime.strptime(opportunity_data.get(col, \"1970-01-01\"), \"%Y-%m-%d\")\n",
    "        days_since_reference = (date_value - datetime(1970, 1, 1)).days\n",
    "        X_num = np.append(X_num, [[days_since_reference]], axis=1)  # Shape: (1, num_numerical_features + num_date_features)\n",
    "\n",
    "    X_text = tokenizer.texts_to_sequences([opportunity_data.get(text_features[0], \"\")])\n",
    "    X_text = pad_sequences(X_text, maxlen=max_len)  # Shape: (1, max_len)\n",
    "\n",
    "    # Combine opportunity data with each title's features\n",
    "    X_cat_all = np.tile(X_cat, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "    X_num_all = np.tile(X_num, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "\n",
    "    X_text_all = []\n",
    "    for title_synopsis in titles_df['title_synopsis']:\n",
    "        seq = tokenizer.texts_to_sequences([title_synopsis])\n",
    "        padded_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        X_text_all.append(padded_seq[0])\n",
    "    X_text_all = np.array(X_text_all)  # Shape: (num_titles, max_len)\n",
    "\n",
    "    return X_cat_all, X_num_all, X_text_all\n",
    "\n",
    "# Opportunity data\n",
    "opportunity_data = {\n",
    "    \"opportunity_name\": \"Example Opportunity\",\n",
    "    \"opportunity_geography_type\": \"Region\",\n",
    "    \"opportunity_reporting_territory\": \"Territory\",\n",
    "    \"opp_start_date\": \"2023-01-01\",\n",
    "    \"opp_expected_close_date\": \"2023-12-31\",\n",
    "    \"title_synopsis\": \"This is a synopsis of the title.\"\n",
    "}\n",
    "\n",
    "# Preprocess opportunity data\n",
    "X_cat_all, X_num_all, X_text_all = preprocess_opportunity_data(\n",
    "    opportunity_data, titles, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X_cat_all, X_num_all, X_text_all])\n",
    "\n",
    "# Add predictions to titles dataframe\n",
    "titles['relevance_score'] = predictions\n",
    "\n",
    "# Sort titles by relevance score\n",
    "sorted_titles = titles.sort_values(by='relevance_score', ascending=False)\n",
    "\n",
    "# Display top 10 titles\n",
    "print(\"Top 10 Titles for the Opportunity:\")\n",
    "print(sorted_titles[['title_name', 'relevance_score']].head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba49f85c-43a9-4d62-a39c-bff7aaa25d0e",
   "metadata": {},
   "source": [
    "# Deploy the model\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef6d6d36-c392-4d64-904a-fa250b54fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archive uploaded to: s3://titlesbucket/opportunity-title-prediction/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "# File paths\n",
    "model_file = 'model.h5'\n",
    "label_encoders_file = 'label_encoders.pkl'\n",
    "package_dir = 'model_package'\n",
    "tar_gz_file = 'model.tar.gz'\n",
    "bucket_name = 'titlesbucket'\n",
    "s3_model_path = f's3://{bucket_name}/opportunity-title-prediction/{tar_gz_file}'\n",
    "\n",
    "# 1. Create a folder to package the model\n",
    "if not os.path.exists(package_dir):\n",
    "    os.makedirs(package_dir)\n",
    "\n",
    "# 2. Copy the model and other necessary files to the package folder\n",
    "shutil.copy(model_file, package_dir)\n",
    "shutil.copy(label_encoders_file, package_dir)\n",
    "\n",
    "# 3. Create a .tar.gz archive from the model_package folder\n",
    "shutil.make_archive(tar_gz_file.replace('.tar.gz', ''), 'gztar', package_dir)\n",
    "\n",
    "# 4. Upload the .tar.gz archive to S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(tar_gz_file, bucket_name, f'opportunity-title-prediction/{tar_gz_file}')\n",
    "\n",
    "# 5. Confirm the file is uploaded\n",
    "print(f\"Model archive uploaded to: {s3_model_path}\")\n",
    "\n",
    "# Clean up: Remove the temporary directory and archive\n",
    "#shutil.rmtree(package_dir)\n",
    "#os.remove(tar_gz_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb7bc4-8b6a-4c9f-bbe0-fb5fb4a4fc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806ba2f-7a1a-40bb-a262-4cf7d05decde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "01b91f46-fd83-4d5f-94c9-29160b18a438",
   "metadata": {},
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Path to your model.tar.gz in S3\n",
    "model_data = \"s3://titlesbucket/opportunity-title-prediction/model.tar.gz\"\n",
    "\n",
    "# Instantiate a TensorFlow model\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    framework_version='2.13.0'  # TensorFlow version\n",
    ")\n",
    "#entry_point = \n",
    "\n",
    "# Deploy the model\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45158513-72fa-44d5-818d-eb5b15ee0203",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load data from CSV files\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "\n",
    "# Features\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"]\n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Load pre-trained model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Load preprocessing objects (label encoders, scaler, tokenizer)\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merged_df[numerical_features])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "\n",
    "# Function to preprocess opportunity data\n",
    "def preprocess_opportunity_data(opportunity_data, titles_df, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer, max_len=100):\n",
    "    \"\"\"\n",
    "    Preprocess opportunity data and combine it with each title's features.\n",
    "    Handles unseen categorical labels by mapping them to an \"unknown\" category.\n",
    "    \"\"\"\n",
    "    # Preprocess opportunity data\n",
    "    X_cat = []\n",
    "    for col in categorical_features:\n",
    "        le = label_encoders[col]\n",
    "        # Handle unseen labels by mapping them to \"unknown\"\n",
    "        if opportunity_data.get(col, \"unknown\") in le.classes_:\n",
    "            X_cat.append(le.transform([opportunity_data.get(col, \"unknown\")])[0])\n",
    "        else:\n",
    "            # Map unseen labels to \"unknown\"\n",
    "            X_cat.append(le.transform([\"unknown\"])[0])\n",
    "    X_cat = np.array(X_cat).reshape(1, -1)  # Shape: (1, num_categorical_features)\n",
    "\n",
    "    X_num = np.array([[opportunity_data.get(col, 0) for col in numerical_features]])\n",
    "    X_num = scaler.transform(X_num)  # Scale numerical features\n",
    "\n",
    "    for col in date_features:\n",
    "        date_value = datetime.strptime(opportunity_data.get(col, \"1970-01-01\"), \"%Y-%m-%d\")\n",
    "        days_since_reference = (date_value - datetime(1970, 1, 1)).days\n",
    "        X_num = np.append(X_num, [[days_since_reference]], axis=1)  # Shape: (1, num_numerical_features + num_date_features)\n",
    "\n",
    "    X_text = tokenizer.texts_to_sequences([opportunity_data.get(text_features[0], \"\")])\n",
    "    X_text = pad_sequences(X_text, maxlen=max_len)  # Shape: (1, max_len)\n",
    "\n",
    "    # Combine opportunity data with each title's features\n",
    "    X_cat_all = np.tile(X_cat, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "    X_num_all = np.tile(X_num, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "\n",
    "    X_text_all = []\n",
    "    for title_synopsis in titles_df['title_synopsis']:\n",
    "        seq = tokenizer.texts_to_sequences([title_synopsis])\n",
    "        padded_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        X_text_all.append(padded_seq[0])\n",
    "    X_text_all = np.array(X_text_all)  # Shape: (num_titles, max_len)\n",
    "\n",
    "    return X_cat_all, X_num_all, X_text_all\n",
    "\n",
    "# Opportunity data\n",
    "opportunity_data = {\n",
    "    \"opportunity_name\": \"Example Opportunity\",\n",
    "    \"opportunity_geography_type\": \"Region\",\n",
    "    \"opportunity_reporting_territory\": \"Territory\",\n",
    "    \"opp_start_date\": \"2023-01-01\",\n",
    "    \"opp_expected_close_date\": \"2023-12-31\",\n",
    "    \"title_synopsis\": \"This is a synopsis of the title.\"\n",
    "}\n",
    "\n",
    "# Preprocess opportunity data\n",
    "X_cat_all, X_num_all, X_text_all = preprocess_opportunity_data(\n",
    "    opportunity_data, titles, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X_cat_all, X_num_all, X_text_all])\n",
    "\n",
    "# Add predictions to titles dataframe\n",
    "titles['relevance_score'] = predictions\n",
    "\n",
    "# Sort titles by relevance score\n",
    "sorted_titles = titles.sort_values(by='relevance_score', ascending=False)\n",
    "\n",
    "# Display top 10 titles\n",
    "print(\"Top 10 Titles for the Opportunity:\")\n",
    "print(sorted_titles[['title_name', 'relevance_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd29f28-931e-46fd-b0df-89b9ada32b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOOOOO\n",
    "#role = sagemaker.get_execution_role()\n",
    "\n",
    "# Upload model to S3\n",
    "#agemaker_session = sagemaker.Session()\n",
    "#bucket = 'titlesbucket'\n",
    "#prefix = 'opportunity-title-prediction'\n",
    "#model_data = sagemaker_session.upload_data(path='model.h5', bucket=bucket, key_prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48769b-f0f3-49c3-8990-492287f4bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOOOOOOO\n",
    "# Create SageMaker model\n",
    "#role = sagemaker.get_execution_role()\n",
    "#tensorflow_model = TensorFlowModel(model_data=model_data, role=role, framework_version='2.4.1')\n",
    "\n",
    "# Deploy model\n",
    "#predictor = tensorflow_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "\n",
    "#print('finished')\n",
    "#print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7dbf1-87fd-444c-932c-cdb7be859d0b",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ad2c749-d3db-4705-8fa1-14f27fc1e264",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Prepare the input data\n",
    "input_data = {\n",
    "    'categorical_input': [...],  # Replace with actual data\n",
    "    'numerical_input': [...],    # Replace with actual data\n",
    "    'date_input': [...],         # Replace with actual data\n",
    "    'text_input': [...]          # Replace with actual data\n",
    "}\n",
    "\n",
    "# Call the SageMaker endpoint\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName='your-endpoint-name',\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(input_data)\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "predictions = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec472b2c-8d72-4620-9973-e72a4b61e023",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the SavedModel\n",
    "model = tf.saved_model.load('saved_model')\n",
    "\n",
    "# Inspect the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3190bb-4ac0-4d37-97b2-022b12430366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
