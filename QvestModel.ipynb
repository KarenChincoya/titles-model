{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44268a4f-d7bb-463a-aa77-a37204bca75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 20:07:53.793035: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-04 20:07:53.990569: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-02-04 20:07:54.267635: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738699674.501058   21563 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738699674.567357   21563 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-04 20:07:55.187694: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Flatten, Concatenate, Dropout, Normalization, TextVectorization\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ba1f5-c094-47f8-9f3b-ccfa46092c4c",
   "metadata": {},
   "source": [
    "###  1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4c0787-76ff-4580-8bd6-ba9bf58c71c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title_id', 'quote_id', 'qli_fee', 'qli_start_date', 'qli_end_date',\n",
      "       'qli_media_type', 'qli_rights', 'relevance', 'opportunity_id',\n",
      "       'opportunity_name', 'opportunity_deal_amount', 'opp_start_date',\n",
      "       'opp_expected_close_date', 'opportunity_geography_type',\n",
      "       'opportunity_reporting_territory', 'opportunity_stage', 'title_name',\n",
      "       'title_r', 'title_runtime', 'title_primary_genre', 'title_level',\n",
      "       'title_production_year', 'title_synopsis'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV FILES\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "print(merged_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6ff308-02fc-4c65-bb7c-0706c521343b",
   "metadata": {},
   "source": [
    "### 2. Define Features & Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c4b9c39-9593-4809-ace2-f0dfb3f9239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "#opportunity_name\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"] \n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Target: title_id (which title best fits the opportunity)\n",
    "target = \"title_name\"\n",
    "\n",
    "# TO DO: CHECK if there's a better fit for title_synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258bceb-effe-4d99-8542-41614ccbf6bd",
   "metadata": {},
   "source": [
    "### 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d696d090-7df6-4fa3-8de2-a8e339af3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features\n",
    "#label_encoders = {}\n",
    "#for col in categorical_features:\n",
    "#    le = LabelEncoder()\n",
    "#    merged_df[col] = le.fit_transform(merged_df[col])\n",
    "#    label_encoders[col] = le\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "merged_df[numerical_features] = scaler.fit_transform(merged_df[numerical_features])\n",
    "\n",
    "# Handle date features\n",
    "for col in date_features:\n",
    "    merged_df[col] = pd.to_datetime(merged_df[col])\n",
    "    merged_df[col] = (merged_df[col] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1s')\n",
    "\n",
    "# Handle text features\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "sequences = tokenizer.texts_to_sequences(merged_df['title_synopsis'])\n",
    "max_len = 100\n",
    "text_data = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# Separate text data from other features\n",
    "X_other = merged_df[categorical_features + numerical_features + date_features]\n",
    "y = merged_df['relevance']\n",
    "\n",
    "# Split data\n",
    "X_train_other, X_test_other, X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_other, text_data, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "434c1177-016c-4270-a219-cfad61189d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tokenizer.pkl']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(label_encoders, 'label_encoders.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "joblib.dump(tokenizer, 'tokenizer.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1730c8a-a00f-4f2c-9f60-dd47336b49da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save preprocessing objects\n",
    "joblib.dump(label_encoders, 'code/label_encoders.pkl')\n",
    "joblib.dump(scaler, 'code/scaler.pkl')\n",
    "joblib.dump(tokenizer, 'code/tokenizer.pkl')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1f541b-f487-42cd-ad22-49a9b563c51c",
   "metadata": {},
   "source": [
    "### 4. Build Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e07f9b-287d-46cf-9d43-a8632671677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-04 20:08:03.011831: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 221ms/step - accuracy: 0.0000e+00 - loss: 56875880.0000 - val_accuracy: 0.0000e+00 - val_loss: 38509520.0000\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.0000e+00 - loss: 35838108.0000 - val_accuracy: 0.0000e+00 - val_loss: 4183559.2500\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.0000e+00 - loss: 12387024.0000 - val_accuracy: 0.0000e+00 - val_loss: 26701482.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 127ms/step - accuracy: 0.0000e+00 - loss: 23077386.0000 - val_accuracy: 0.0000e+00 - val_loss: 8478322.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.0000e+00 - loss: 7944506.0000 - val_accuracy: 0.0000e+00 - val_loss: 8107619.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 121ms/step - accuracy: 0.0000e+00 - loss: 10862721.0000 - val_accuracy: 0.0000e+00 - val_loss: 2866960.5000\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 119ms/step - accuracy: 0.0000e+00 - loss: 6656181.0000 - val_accuracy: 0.0000e+00 - val_loss: 10237832.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step - accuracy: 0.0000e+00 - loss: 7506178.5000 - val_accuracy: 0.0000e+00 - val_loss: 11765716.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.0000e+00 - loss: 9454907.0000 - val_accuracy: 0.0000e+00 - val_loss: 9635714.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 112ms/step - accuracy: 0.0000e+00 - loss: 8281216.5000 - val_accuracy: 0.0000e+00 - val_loss: 2953669.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f76c27d7f50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate\n",
    "\n",
    "# Define model\n",
    "input_cat = Input(shape=(len(categorical_features),), name=\"input_cat\")\n",
    "input_num = Input(shape=(len(numerical_features) + len(date_features),), name=\"input_num\")\n",
    "input_text = Input(shape=(max_len,), name=\"input_text\")\n",
    "\n",
    "embedding = Embedding(input_dim=10000, output_dim=128)(input_text)\n",
    "lstm = LSTM(64)(embedding)\n",
    "\n",
    "concat = Concatenate()([input_cat, input_num, lstm])\n",
    "dense1 = Dense(128, activation='relu')(concat)\n",
    "output = Dense(1, activation='sigmoid')(dense1)\n",
    "\n",
    "model = Model(inputs=[input_cat, input_num, input_text], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    [X_train_other[categorical_features], X_train_other[numerical_features + date_features], X_train_text],\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5836f3be-a104-4dcd-b20a-9fff6cec4f30",
   "metadata": {},
   "source": [
    "## 5. Deploy to AWS SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ff49c05-0e8c-420d-8b58-8897b633245d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Save the model\n",
    "#model = tf.keras.models.load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68378730-167e-40ff-8b82-fdda2fd7692d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: 1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '1/'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 5), dtype=tf.float32, name='input_cat'), TensorSpec(shape=(None, 7), dtype=tf.float32, name='input_num'), TensorSpec(shape=(None, 100), dtype=tf.float32, name='input_text')]\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140148050553424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148045545744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148045546896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148050552656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148050553808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148050553040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148045547856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140148045548240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Model saved in SavedModel format.\n",
      "['assets', 'variables', 'fingerprint.pb', 'saved_model.pb']\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "#model.save('model.h5')\n",
    "\n",
    "# Save the model in SavedModel format\n",
    "#tf.saved_model.save(model, 'model') #saved model\n",
    "# Define input signatures\n",
    "input_signatures = {\n",
    "    \"input_cat\": tf.TensorSpec(shape=(None, len(categorical_features)), dtype=tf.float32, name=\"input_cat\"),\n",
    "    \"input_num\": tf.TensorSpec(shape=(None, len(numerical_features) + len(date_features)), dtype=tf.float32, name=\"input_num\"),\n",
    "    \"input_text\": tf.TensorSpec(shape=(None, max_len), dtype=tf.int32, name=\"input_text\"),\n",
    "}\n",
    "\n",
    "#model.export(\"1/\") \n",
    "# Export the model\n",
    "#tf.saved_model.save(model, '1/')\n",
    "model.export(\n",
    "    \"1/\"\n",
    "    #input_tensors=input_signatures\n",
    ")\n",
    "print(\"Model saved in SavedModel format.\")\n",
    "\n",
    "# Verify saved structure\n",
    "import os\n",
    "print(os.listdir(\"1/\")) # Should show [\"saved_model.pb\", \"variables\", \"assets\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4770027-e520-49fb-9bfb-618787dfeebf",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define input signatures\n",
    "input_signatures = {\n",
    "    \"input_cat\": tf.TensorSpec(shape=(None, len(categorical_features)), dtype=tf.float32, name=\"input_cat\"),\n",
    "    \"input_num\": tf.TensorSpec(shape=(None, len(numerical_features) + len(date_features)), dtype=tf.float32, name=\"input_num\"),\n",
    "    \"input_text\": tf.TensorSpec(shape=(None, max_len), dtype=tf.int32, name=\"input_text\"),\n",
    "}\n",
    "\n",
    "# Create a concrete function using model.__call__\n",
    "concrete_function = model.__call__.get_concrete_function(**input_signatures)\n",
    "\n",
    "# Export the model\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    \"1/\",  # Directory to save the model\n",
    "    signatures={\n",
    "        \"serving_default\": concrete_function  # Define the serving signature\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Model saved in SavedModel format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8517f071-2f6c-4ea6-964f-d4faf45b76d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# Create a tar.gz file\n",
    "#with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "#    tar.add('model', arcname=os.path.basename('1'))\n",
    "\n",
    "#print(\"Model packaged into model.tar.gz.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d399deac-a9e9-45e2-8561-51f9867a0734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.tar.gz created successfully.\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "# List of files to include in the model.tar.gz\n",
    "files_to_include = [\n",
    "    '1/',  # TensorFlow SavedModel directory saved_model\n",
    "    #'model.h5',\n",
    "    'code/',\n",
    "    #'inference.py',  # Custom inference script\n",
    "    #'requirements.txt',\n",
    "    #'label_encoders.pkl',  # Preprocessing object\n",
    "    #'scaler.pkl',  # Preprocessing object\n",
    "    #'tokenizer.pkl',  # Preprocessing object\n",
    "    #'titles.csv'\n",
    "]\n",
    "\n",
    "# Create the model.tar.gz file\n",
    "#with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "#    for file in files_to_include:\n",
    "#        tar.add(file)\n",
    "\n",
    "\n",
    "# Create the model.tar.gz file\n",
    "with tarfile.open('model.tar.gz', 'w:gz') as tar:\n",
    "    for file in files_to_include:\n",
    "        # If it's the model directory, rename it to just '1' inside the tarball\n",
    "        #if file == '1/':\n",
    "        #    tar.add(file, arcname='1')\n",
    "        #if file == 'code/':\n",
    "        #    tar.add(file, arcname='code')\n",
    "        #else:\n",
    "            tar.add(file)\n",
    "            \n",
    "print(\"model.tar.gz created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832484b6-e727-49f6-9b13-a00693178b44",
   "metadata": {},
   "source": [
    "# 6. Upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b79aa8-c884-41a5-afca-c7b322888b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to s3://titlesbucket/opportunity-title-prediction/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Upload to S3\n",
    "\n",
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "bucket_name = 'titlesbucket'\n",
    "key = 'opportunity-title-prediction/model.tar.gz'\n",
    "\n",
    "# Upload the file\n",
    "#opportunity-title-prediction/model.tar.gz\n",
    "s3.upload_file('model.tar.gz', bucket_name, key)\n",
    "\n",
    "print(f\"Model uploaded to s3://{bucket_name}/{key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91e47098-13a8-46cd-be71-9b8eeb2f30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# Get the SageMaker execution role\n",
    "role = get_execution_role()\n",
    "\n",
    "# Path to your new model.tar.gz in S3\n",
    "# opportunity-title-prediction\n",
    "model_data = \"s3://titlesbucket/opportunity-title-prediction/model.tar.gz\"\n",
    "\n",
    "# Instantiate a TensorFlow model\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    entry_point='inference.py',\n",
    "    source_dir='code',\n",
    "    framework_version='2.13.0',  # TensorFlow version\n",
    "    name='Titles-Prediction-Model-21'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "561c353c-ccea-478f-a815-0df22901a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::779846812857:role/service-role/AmazonSageMaker-ExecutionRole-20250126T233747\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa2a30e-0357-4659-aeed-2c9ad6db30d9",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a190eb8e-c5b4-4388-ad6a-777edf0b0043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SageMaker client\n",
    "#sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Delete the existing endpoint configuration\n",
    "#sagemaker_client.delete_endpoint_config(EndpointConfigName='titles-prediction-my-endpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65084b1a-8810-4cd1-a527-beeb5d06a5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------*"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error hosting endpoint Titles-Prediction-Model-21-2025-02-04-20-09-16-504232: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[1;32m      5\u001b[0m ENDPOINT_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitles-Prediction-Model-21-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(datetime\u001b[38;5;241m.\u001b[39mutcnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS-\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mtensorflow_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mml.c5.large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mENDPOINT_NAME\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#update_endpoint=True\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel deployment finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictor)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/tensorflow/model.py:364\u001b[0m, in \u001b[0;36mTensorFlowModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe TensorFlow version \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support EIA.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework_version\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(msg)\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mTensorFlowModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_instance_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_instance_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43minstance_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstance_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeserializer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformat_tags\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserverless_inference_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserverless_inference_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvolume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvolume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_data_download_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontainer_startup_health_check_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_recommendation_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_recommendation_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/model.py:1749\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, async_inference_config, serverless_inference_config, volume_size, model_data_download_timeout, container_startup_health_check_timeout, inference_recommendation_id, explainer_config, accept_eula, endpoint_logging, resources, endpoint_type, managed_instance_scaling, inference_component_name, routing_config, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_explainer_enabled:\n\u001b[1;32m   1747\u001b[0m     explainer_config_dict \u001b[38;5;241m=\u001b[39m explainer_config\u001b[38;5;241m.\u001b[39m_to_request_dict()\n\u001b[0;32m-> 1749\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_from_production_variants\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproduction_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mproduction_variant\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkms_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkms_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_capture_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplainer_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43masync_inference_config_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls:\n\u001b[1;32m   1762\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor_cls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_session)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5728\u001b[0m, in \u001b[0;36mSession.endpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict, async_inference_config_dict, explainer_config_dict, live_logging, vpc_config, enable_network_isolation, role)\u001b[0m\n\u001b[1;32m   5725\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating endpoint-config with name \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[1;32m   5726\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_endpoint_config(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_options)\n\u001b[0;32m-> 5728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5729\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5734\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:4586\u001b[0m, in \u001b[0;36mSession.create_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait, live_logging)\u001b[0m\n\u001b[1;32m   4583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendpoint_arn \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEndpointArn\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   4585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m-> 4586\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendpoint_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlive_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlive_logging\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m endpoint_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/sagemaker/session.py:5371\u001b[0m, in \u001b[0;36mSession.wait_for_endpoint\u001b[0;34m(self, endpoint, poll, live_logging)\u001b[0m\n\u001b[1;32m   5365\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCapacityError\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(reason):\n\u001b[1;32m   5366\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mCapacityError(\n\u001b[1;32m   5367\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5368\u001b[0m             allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5369\u001b[0m             actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5370\u001b[0m         )\n\u001b[0;32m-> 5371\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mUnexpectedStatusException(\n\u001b[1;32m   5372\u001b[0m         message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m   5373\u001b[0m         allowed_statuses\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInService\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   5374\u001b[0m         actual_status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[1;32m   5375\u001b[0m     )\n\u001b[1;32m   5376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m desc\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error hosting endpoint Titles-Prediction-Model-21-2025-02-04-20-09-16-504232: Failed. Reason: The primary container for production variant AllTraffic did not pass the ping health check. Please check CloudWatch logs for this endpoint.. Try changing the instance type or reference the troubleshooting page https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference-troubleshooting.html"
     ]
    }
   ],
   "source": [
    "# Deploy the model\n",
    "# m5\n",
    "from datetime import datetime\n",
    "\n",
    "ENDPOINT_NAME = 'Titles-Prediction-Model-21-' + str(datetime.utcnow().strftime('%Y-%m-%d-%H-%M-%S-%f'))\n",
    "\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.large',  \n",
    "    endpoint_name=ENDPOINT_NAME \n",
    "    #update_endpoint=True\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0aa70bdf-af08-4019-a587-ecfb8714198b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/ml/model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[188], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/opt/ml/model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Should show ['1', 'code']\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/ml/model/1\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Should show model files\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/opt/ml/model/code\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# Should show inference.py, encoders, scaler, tokenizer, etc.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/ml/model'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"/opt/ml/model\"))  # Should show ['1', 'code']\n",
    "print(os.listdir(\"/opt/ml/model/1\"))  # Should show model files\n",
    "print(os.listdir(\"/opt/ml/model/code\"))  # Should show inference.py, encoders, scaler, tokenizer, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc1672-b685-4856-b6c3-0b1212f15235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load data from CSV files\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "\n",
    "# Features\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"]\n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Load pre-trained model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Load preprocessing objects (label encoders, scaler, tokenizer)\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merged_df[numerical_features])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "\n",
    "# Function to preprocess opportunity data\n",
    "def preprocess_opportunity_data(opportunity_data, titles_df, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer, max_len=100):\n",
    "    \"\"\"\n",
    "    Preprocess opportunity data and combine it with each title's features.\n",
    "    Handles unseen categorical labels by mapping them to an \"unknown\" category.\n",
    "    \"\"\"\n",
    "    # Preprocess opportunity data\n",
    "    X_cat = []\n",
    "    for col in categorical_features:\n",
    "        le = label_encoders[col]\n",
    "        # Handle unseen labels by mapping them to \"unknown\"\n",
    "        if opportunity_data.get(col, \"unknown\") in le.classes_:\n",
    "            X_cat.append(le.transform([opportunity_data.get(col, \"unknown\")])[0])\n",
    "        else:\n",
    "            # Map unseen labels to \"unknown\"\n",
    "            X_cat.append(le.transform([\"unknown\"])[0])\n",
    "    X_cat = np.array(X_cat).reshape(1, -1)  # Shape: (1, num_categorical_features)\n",
    "\n",
    "    X_num = np.array([[opportunity_data.get(col, 0) for col in numerical_features]])\n",
    "    X_num = scaler.transform(X_num)  # Scale numerical features\n",
    "\n",
    "    for col in date_features:\n",
    "        date_value = datetime.strptime(opportunity_data.get(col, \"1970-01-01\"), \"%Y-%m-%d\")\n",
    "        days_since_reference = (date_value - datetime(1970, 1, 1)).days\n",
    "        X_num = np.append(X_num, [[days_since_reference]], axis=1)  # Shape: (1, num_numerical_features + num_date_features)\n",
    "\n",
    "    X_text = tokenizer.texts_to_sequences([opportunity_data.get(text_features[0], \"\")])\n",
    "    X_text = pad_sequences(X_text, maxlen=max_len)  # Shape: (1, max_len)\n",
    "\n",
    "    # Combine opportunity data with each title's features\n",
    "    X_cat_all = np.tile(X_cat, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "    X_num_all = np.tile(X_num, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "\n",
    "    X_text_all = []\n",
    "    for title_synopsis in titles_df['title_synopsis']:\n",
    "        seq = tokenizer.texts_to_sequences([title_synopsis])\n",
    "        padded_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        X_text_all.append(padded_seq[0])\n",
    "    X_text_all = np.array(X_text_all)  # Shape: (num_titles, max_len)\n",
    "\n",
    "    return X_cat_all, X_num_all, X_text_all\n",
    "\n",
    "# Opportunity data\n",
    "opportunity_data = {\n",
    "    \"opportunity_name\": \"Example Opportunity\",\n",
    "    \"opportunity_geography_type\": \"Region\",\n",
    "    \"opportunity_reporting_territory\": \"Territory\",\n",
    "    \"opp_start_date\": \"2023-01-01\",\n",
    "    \"opp_expected_close_date\": \"2023-12-31\",\n",
    "    \"title_synopsis\": \"This is a synopsis of the title.\"\n",
    "}\n",
    "\n",
    "# Preprocess opportunity data\n",
    "X_cat_all, X_num_all, X_text_all = preprocess_opportunity_data(\n",
    "    opportunity_data, titles, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X_cat_all, X_num_all, X_text_all])\n",
    "\n",
    "# Add predictions to titles dataframe\n",
    "titles['relevance_score'] = predictions\n",
    "\n",
    "# Sort titles by relevance score\n",
    "sorted_titles = titles.sort_values(by='relevance_score', ascending=False)\n",
    "\n",
    "# Display top 10 titles\n",
    "print(\"Top 10 Titles for the Opportunity:\")\n",
    "print(sorted_titles[['title_name', 'relevance_score']].head(10))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba49f85c-43a9-4d62-a39c-bff7aaa25d0e",
   "metadata": {},
   "source": [
    "# Deploy the model\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef6d6d36-c392-4d64-904a-fa250b54fe82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model archive uploaded to: s3://titlesbucket/opportunity-title-prediction/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import boto3\n",
    "\n",
    "# File paths\n",
    "model_file = 'model.h5'\n",
    "label_encoders_file = 'label_encoders.pkl'\n",
    "package_dir = 'model_package'\n",
    "tar_gz_file = 'model.tar.gz'\n",
    "bucket_name = 'titlesbucket'\n",
    "s3_model_path = f's3://{bucket_name}/opportunity-title-prediction/{tar_gz_file}'\n",
    "\n",
    "# 1. Create a folder to package the model\n",
    "if not os.path.exists(package_dir):\n",
    "    os.makedirs(package_dir)\n",
    "\n",
    "# 2. Copy the model and other necessary files to the package folder\n",
    "shutil.copy(model_file, package_dir)\n",
    "shutil.copy(label_encoders_file, package_dir)\n",
    "\n",
    "# 3. Create a .tar.gz archive from the model_package folder\n",
    "shutil.make_archive(tar_gz_file.replace('.tar.gz', ''), 'gztar', package_dir)\n",
    "\n",
    "# 4. Upload the .tar.gz archive to S3\n",
    "s3_client = boto3.client('s3')\n",
    "s3_client.upload_file(tar_gz_file, bucket_name, f'opportunity-title-prediction/{tar_gz_file}')\n",
    "\n",
    "# 5. Confirm the file is uploaded\n",
    "print(f\"Model archive uploaded to: {s3_model_path}\")\n",
    "\n",
    "# Clean up: Remove the temporary directory and archive\n",
    "#shutil.rmtree(package_dir)\n",
    "#os.remove(tar_gz_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecb7bc4-8b6a-4c9f-bbe0-fb5fb4a4fc81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806ba2f-7a1a-40bb-a262-4cf7d05decde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "01b91f46-fd83-4d5f-94c9-29160b18a438",
   "metadata": {},
   "source": [
    "from sagemaker.tensorflow import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "# Path to your model.tar.gz in S3\n",
    "model_data = \"s3://titlesbucket/opportunity-title-prediction/model.tar.gz\"\n",
    "\n",
    "# Instantiate a TensorFlow model\n",
    "tensorflow_model = TensorFlowModel(\n",
    "    model_data=model_data,\n",
    "    role=role,\n",
    "    framework_version='2.13.0'  # TensorFlow version\n",
    ")\n",
    "#entry_point = \n",
    "\n",
    "# Deploy the model\n",
    "predictor = tensorflow_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large'\n",
    ")\n",
    "\n",
    "print('Model deployment finished')\n",
    "print(predictor)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45158513-72fa-44d5-818d-eb5b15ee0203",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load data from CSV files\n",
    "titles = pd.read_csv(\"data/Titles.csv\")\n",
    "opportunities = pd.read_csv(\"data/Opportunities.csv\")\n",
    "quotes = pd.read_csv(\"data/Quotes.csv\")\n",
    "quote_line_items = pd.read_csv(\"data/Quote_Line_Items.csv\")\n",
    "\n",
    "# Merge datasets\n",
    "quote_qlis = quote_line_items.merge(quotes, on='quote_id')\n",
    "opp_quote_qlis = quote_qlis.merge(opportunities, on='opportunity_id')\n",
    "data = opp_quote_qlis.merge(titles, on='title_id')\n",
    "\n",
    "merged_df = data\n",
    "\n",
    "# Features\n",
    "categorical_features = [\"opportunity_geography_type\", \"opportunity_reporting_territory\", \"qli_media_type\", \"qli_rights\", \"title_primary_genre\"]\n",
    "numerical_features = [\"opportunity_deal_amount\", \"title_runtime\", \"title_production_year\"]\n",
    "date_features = [\"opp_start_date\", \"opp_expected_close_date\", \"qli_start_date\", \"qli_end_date\"]\n",
    "text_features = [\"title_synopsis\"]\n",
    "\n",
    "# Load pre-trained model\n",
    "model = load_model('model.h5')\n",
    "\n",
    "# Load preprocessing objects (label encoders, scaler, tokenizer)\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    # Add \"unknown\" category to the encoder\n",
    "    merged_df[col] = merged_df[col].astype(str)  # Ensure all values are strings\n",
    "    le.fit(list(merged_df[col].unique()) + [\"unknown\"])  # Add \"unknown\" to the encoder\n",
    "    merged_df[col] = le.transform(merged_df[col])\n",
    "    label_encoders[col] = le\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(merged_df[numerical_features])\n",
    "\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(merged_df['title_synopsis'])\n",
    "\n",
    "# Function to preprocess opportunity data\n",
    "def preprocess_opportunity_data(opportunity_data, titles_df, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer, max_len=100):\n",
    "    \"\"\"\n",
    "    Preprocess opportunity data and combine it with each title's features.\n",
    "    Handles unseen categorical labels by mapping them to an \"unknown\" category.\n",
    "    \"\"\"\n",
    "    # Preprocess opportunity data\n",
    "    X_cat = []\n",
    "    for col in categorical_features:\n",
    "        le = label_encoders[col]\n",
    "        # Handle unseen labels by mapping them to \"unknown\"\n",
    "        if opportunity_data.get(col, \"unknown\") in le.classes_:\n",
    "            X_cat.append(le.transform([opportunity_data.get(col, \"unknown\")])[0])\n",
    "        else:\n",
    "            # Map unseen labels to \"unknown\"\n",
    "            X_cat.append(le.transform([\"unknown\"])[0])\n",
    "    X_cat = np.array(X_cat).reshape(1, -1)  # Shape: (1, num_categorical_features)\n",
    "\n",
    "    X_num = np.array([[opportunity_data.get(col, 0) for col in numerical_features]])\n",
    "    X_num = scaler.transform(X_num)  # Scale numerical features\n",
    "\n",
    "    for col in date_features:\n",
    "        date_value = datetime.strptime(opportunity_data.get(col, \"1970-01-01\"), \"%Y-%m-%d\")\n",
    "        days_since_reference = (date_value - datetime(1970, 1, 1)).days\n",
    "        X_num = np.append(X_num, [[days_since_reference]], axis=1)  # Shape: (1, num_numerical_features + num_date_features)\n",
    "\n",
    "    X_text = tokenizer.texts_to_sequences([opportunity_data.get(text_features[0], \"\")])\n",
    "    X_text = pad_sequences(X_text, maxlen=max_len)  # Shape: (1, max_len)\n",
    "\n",
    "    # Combine opportunity data with each title's features\n",
    "    X_cat_all = np.tile(X_cat, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "    X_num_all = np.tile(X_num, (len(titles_df), 1))  # Repeat opportunity data for each title\n",
    "\n",
    "    X_text_all = []\n",
    "    for title_synopsis in titles_df['title_synopsis']:\n",
    "        seq = tokenizer.texts_to_sequences([title_synopsis])\n",
    "        padded_seq = pad_sequences(seq, maxlen=max_len)\n",
    "        X_text_all.append(padded_seq[0])\n",
    "    X_text_all = np.array(X_text_all)  # Shape: (num_titles, max_len)\n",
    "\n",
    "    return X_cat_all, X_num_all, X_text_all\n",
    "\n",
    "# Opportunity data\n",
    "opportunity_data = {\n",
    "    \"opportunity_name\": \"Example Opportunity\",\n",
    "    \"opportunity_geography_type\": \"Region\",\n",
    "    \"opportunity_reporting_territory\": \"Territory\",\n",
    "    \"opp_start_date\": \"2023-01-01\",\n",
    "    \"opp_expected_close_date\": \"2023-12-31\",\n",
    "    \"title_synopsis\": \"This is a synopsis of the title.\"\n",
    "}\n",
    "\n",
    "# Preprocess opportunity data\n",
    "X_cat_all, X_num_all, X_text_all = preprocess_opportunity_data(\n",
    "    opportunity_data, titles, categorical_features, numerical_features, date_features, text_features, label_encoders, scaler, tokenizer\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X_cat_all, X_num_all, X_text_all])\n",
    "\n",
    "# Add predictions to titles dataframe\n",
    "titles['relevance_score'] = predictions\n",
    "\n",
    "# Sort titles by relevance score\n",
    "sorted_titles = titles.sort_values(by='relevance_score', ascending=False)\n",
    "\n",
    "# Display top 10 titles\n",
    "print(\"Top 10 Titles for the Opportunity:\")\n",
    "print(sorted_titles[['title_name', 'relevance_score']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd29f28-931e-46fd-b0df-89b9ada32b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOOOOO\n",
    "#role = sagemaker.get_execution_role()\n",
    "\n",
    "# Upload model to S3\n",
    "#agemaker_session = sagemaker.Session()\n",
    "#bucket = 'titlesbucket'\n",
    "#prefix = 'opportunity-title-prediction'\n",
    "#model_data = sagemaker_session.upload_data(path='model.h5', bucket=bucket, key_prefix=prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d48769b-f0f3-49c3-8990-492287f4bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOOOOOOO\n",
    "# Create SageMaker model\n",
    "#role = sagemaker.get_execution_role()\n",
    "#tensorflow_model = TensorFlowModel(model_data=model_data, role=role, framework_version='2.4.1')\n",
    "\n",
    "# Deploy model\n",
    "#predictor = tensorflow_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "\n",
    "#print('finished')\n",
    "#print(predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f7dbf1-87fd-444c-932c-cdb7be859d0b",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0ad2c749-d3db-4705-8fa1-14f27fc1e264",
   "metadata": {},
   "source": [
    "import boto3\n",
    "\n",
    "# Create a SageMaker runtime client\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Prepare the input data\n",
    "input_data = {\n",
    "    'categorical_input': [...],  # Replace with actual data\n",
    "    'numerical_input': [...],    # Replace with actual data\n",
    "    'date_input': [...],         # Replace with actual data\n",
    "    'text_input': [...]          # Replace with actual data\n",
    "}\n",
    "\n",
    "# Call the SageMaker endpoint\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName='your-endpoint-name',\n",
    "    ContentType='application/json',\n",
    "    Body=json.dumps(input_data)\n",
    ")\n",
    "\n",
    "# Parse the response\n",
    "predictions = json.loads(response['Body'].read().decode())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec472b2c-8d72-4620-9973-e72a4b61e023",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Load the SavedModel\n",
    "model = tf.saved_model.load('saved_model')\n",
    "\n",
    "# Inspect the model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3190bb-4ac0-4d37-97b2-022b12430366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
